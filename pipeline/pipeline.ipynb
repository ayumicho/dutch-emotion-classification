{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec5da8f1",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "This pipeline is designed to download Dutch audio from a YouTube video, transcibe it to English, classify the emotion per sentence and save the results in a CSV file with timestamps per sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "411f1646-ce87-4cd8-9a6b-ec8d5ead0336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import torch\n",
    "import whisper\n",
    "import requests\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    MarianMTModel,\n",
    "    MarianTokenizer\n",
    ")\n",
    "from datetime import datetime\n",
    "from pytubefix import YouTube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e496bc3c-bebd-45ac-bad9-dfd955ef9df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the API\n",
    "TOKEN = \"sk-75f81de4ee9e4eb1ba31a0b6f7721560\"\n",
    "API_BASE = \"http://194.171.191.228:30080/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d6656c7-220d-4c87-8a35-e2de54073b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define emotion mapping\n",
    "emotions = [\"neutral\", \"happiness\", \"anger\", \"fear\", \"sadness\", \"disgust\", \"surprise\"]\n",
    "emotion_mapping = \"\"\"\n",
    "    happiness: \"joy\", \"optimism\", \"approval\", \"pride\", \"gratitude\", \"confidence\",\n",
    "                \"satisfaction\", \"hope\", \"love\", \"excitement\", \"caring\", \"relief\", \"admiration\",\n",
    "                \"amusement\", \"anticipation\", \"encouragement\", \"desire\", \"happiness\"\n",
    "    sadness: \"disappointment\", \"nostalgia\", \"remorse\", \"pain\", \"stress\", \"regret\",\n",
    "              \"resignation\", \"despair\", \"confusion\", \"uncertainty\", \"sadness\"\n",
    "    anger: \"anger\", \"annoyance\", \"disapproval\", \"frustration\", \"disbelief\",\n",
    "            \"warning\", \"rejection\"\n",
    "    disgust: \"disgust\"\n",
    "    fear: \"fear\", \"nervousness\", \"worry\", \"anxiety\", \"doubt\", \"insecurity\", \n",
    "           \"urgency\", \"panic\"\n",
    "    surprise: \"surprise\", \"realization\", \"shock\", \"amazement\", \"wonder\"\n",
    "    neutral: \"neutral\", \"mixed\", \"trust\", \"agreement\", \"instruction\", \"suggestion\",\n",
    "              \"request\", \"confirmation\", \"acceptance\", \"reassurance\", \"clarification\",\n",
    "              \"understanding\", \"certainty\", \"curiosity\"\n",
    "\"\"\"\n",
    "\n",
    "EMO_PROMPT = f\"\"\"You will analyze sentences and classify emotions that best relate to the sentence. Please follow these steps:\n",
    "    1. Identify the primary emotion expressed in the sentence. \n",
    "    2. Classify the sentences into the main emotions: {emotions}\n",
    "    3. If trouble classifying refer to this mapping: {emotion_mapping}\n",
    "    4. Only provide the MAIN emotion as the answer.\n",
    "    5. You are NOT allowed to use more than one word as the answer.\n",
    "    6. Do not give me the reasoning.\n",
    "    7. If the sentence is unclear or the emotion is not strong, return \"neutral\".\n",
    "    8. Only these 7 types of outputs are allowed: {emotions}\n",
    "\n",
    "    Here is the sentence to analyze:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614ea432-7d5a-4ad0-88b0-a33da9a7a574",
   "metadata": {},
   "outputs": [],
   "source": [
    "def youtube_to_audio(youtube_url, output_path=\".\"):\n",
    "    \"\"\"\n",
    "    Download audio from YouTube and save as MP3.\n",
    "    Args:\n",
    "        youtube_url (str): YouTube video URL.\n",
    "        audio_file (str): Output MP3 file path.\n",
    "    Returns:\n",
    "        str: Path to the downloaded MP3 file.\n",
    "    \"\"\"\n",
    "    youtube = YouTube(youtube_url)\n",
    "    video_file = youtube.streams.filter(only_audio=True).first()\n",
    "    audio_file = video_file.download(output_path=output_path)\n",
    "    mp3_file = os.path.splitext(audio_file)[0] + \".mp3\"\n",
    "    os.rename(audio_file, mp3_file)\n",
    "    print(f\"Audio downloaded to {audio_file}\")\n",
    "    return mp3_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3238095d-a4e0-4867-bb17-606b2d719275",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_timestamp(seconds: float) -> str:\n",
    "    \"\"\"\n",
    "    Convert seconds to SRT timestamp format (HH:MM:SS,mmm).\n",
    "    Args:\n",
    "        seconds (float): Timestamp in seconds.\n",
    "    Returns:\n",
    "        str: Formatted timestamp (e.g., \"00:00:02,000\").\n",
    "    \"\"\"\n",
    "    return datetime.utcfromtimestamp(seconds).strftime('%H:%M:%S,%f')[:-3]\n",
    "\n",
    "def audio_transcription(input_file: str, \n",
    "                     output_path: str = \"transcription.csv\",\n",
    "                     language: str = \"nl\"):\n",
    "    \"\"\"\n",
    "    Transcribing audio and save with timestamps.\n",
    "    Args:\n",
    "        input_file (str): Path to the input audio file.\n",
    "        output_path (str): Path to the output CSV file (default: \"transcription.csv\").\n",
    "        language (str): Language code (e.g., \"nl\" for Dutch).\n",
    "    Returns:\n",
    "        dict: Full transcription result from Whisper.\n",
    "    \"\"\"\n",
    "    model = whisper.load_model(\"large-v3\")\n",
    "    transcription = model.transcribe(input_file, language=language)\n",
    "    \n",
    "    # Write to CSV with timestamps\n",
    "    with open(output_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Start Time\", \"End Time\", \"Sentence\"])\n",
    "        \n",
    "        for segment in transcription['segments']:\n",
    "            start_time = format_timestamp(segment['start'])\n",
    "            end_time = format_timestamp(segment['end'])\n",
    "            sentence = segment['text'].strip()\n",
    "            writer.writerow([start_time, end_time, sentence])\n",
    "    print(f\"Transcription with timestamps saved to {output_path}\")  \n",
    "    return transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3adddb-a416-4191-b513-3b16cb3f3188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path):\n",
    "    \"\"\"\n",
    "    Load MarianMT translation model and tokenizer.\n",
    "    Args:\n",
    "        model_path (str): Path or name of the translation model.\n",
    "    Returns:\n",
    "        tuple: (tokenizer, model)\n",
    "    \"\"\"\n",
    "    tokenizer = MarianTokenizer.from_pretrained(model_path)\n",
    "    model = MarianMTModel.from_pretrained(model_path)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    return tokenizer, model\n",
    "    \n",
    "def audio_translation(text, tokenizer, model):\n",
    "    \"\"\"\n",
    "    Translate text using MarianMT model.\n",
    "    Args:\n",
    "        text (str): Text to translate.\n",
    "        tokenizer: MarianMT tokenizer.\n",
    "        model: MarianMT model.\n",
    "    Returns:\n",
    "        str: Translated text.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        translated_ids = model.generate(**inputs)\n",
    "    return tokenizer.decode(translated_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "def emotion_classification(sentence):\n",
    "    \"\"\"\n",
    "    Classify emotion of a sentence using the API.\n",
    "    Args:\n",
    "        sentence (str): Sentence to classify.\n",
    "    Returns:\n",
    "        str: Classified emotion. If classification fails, returns 'neutral'.\n",
    "    \"\"\"\n",
    "    url = f'{API_BASE}/api/chat/completions'\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {TOKEN}',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    prompt = [{\"role\": \"system\", \"content\": EMO_PROMPT},\n",
    "              {\"role\": \"user\", \"content\": sentence}]\n",
    "    \n",
    "    response = requests.post(url, headers=headers, json={\"model\": \"llama3.2:3b\", \"messages\": prompt})    \n",
    "    if response.status_code == 200:\n",
    "        return response.json().get('choices', [{}])[0].get('message', {}).get('content', 'neutral')\n",
    "    return 'neutral'\n",
    "\n",
    "def save_transcription(sentences, translations, emotions, output_file=\"Pipeline.csv\"):\n",
    "    \"\"\"\n",
    "    Save transcription, translations, and emotions to CSV.\n",
    "    Args:\n",
    "        sentences (list): List of tuples (start, end, sentence).\n",
    "        translations (list): List of translated sentences.\n",
    "        emotions (list): List of classified emotions.\n",
    "        output_file (str): Output CSV file path (default: \"Pipeline.csv\").\n",
    "    Returns:\n",
    "        str: Path to the saved CSV file.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(sentences, columns=[\"Start_Time\", \"End_Time\", \"Sentence\"])\n",
    "    df[\"Translation\"] = translations\n",
    "    df[\"Emotion\"] = emotions\n",
    "    df.to_csv(output_file, index=False)\n",
    "    return output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190eb133-9f79-4cfc-9f24-3615885c7f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(youtube_url: str, model_path: str, output_csv: str = \"pipeline_output.csv\"):\n",
    "    \"\"\"\n",
    "    This pipeline function orchestrates the entire process of:\n",
    "    - downloading audio from a YouTube video\n",
    "    - transcribing it using Whisper\n",
    "    - translating the transcribed text\n",
    "    - classifying emotions\n",
    "    - saving the results to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        youtube_url (str): YouTube video URL.\n",
    "        model_path (str): Path to translation model.\n",
    "        output_csv (str): Final output CSV path.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Extract audio from the YouTube video\n",
    "    audio_file = youtube_to_audio(youtube_url)\n",
    "    \n",
    "    # 2. Generate transcriptions using Whisper\n",
    "    transcription = audio_transcription(audio_file)\n",
    "    \n",
    "    # 3. Parse and format the transcription into sentence segments\n",
    "    sentences = [\n",
    "        (format_timestamp(segment['start']),\n",
    "         format_timestamp(segment['end']),\n",
    "         segment['text'].strip())\n",
    "        for segment in transcription['segments']\n",
    "    ]\n",
    "    \n",
    "    # 4. Load the translation model and tokenizer\n",
    "    tokenizer, model = load_model(model_path)\n",
    "    \n",
    "    # 5. Perform translation and emotion analysis for each sentence\n",
    "    translations = [audio_translation(text, tokenizer, model) for _, _, text in sentences]\n",
    "    emotions = [emotion_classification(text) for _, _, text in sentences]\n",
    "\n",
    "    # 6. Save all processed data to a CSV file\n",
    "    output_file = save_transcription(sentences, translations, emotions, output_csv)\n",
    "    print(f\"Transcription and classification saved to file {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc01e58e-07bc-4293-a815-0db15b9401fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio downloaded to /home/y3/'ZEIKENDE' buurvrouw 'probeert PARKEERTUIG AAN TE RIJDEN!' | Mr. Frank Visser doet uitspraak #AFL123.m4a\n",
      "Transcription with timestamps saved to transcription.csv\n",
      "Transcription and classification saved to file pipeline_output.csv\n"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "youtube_url = \"https://youtu.be/TWIgWhJZJgo?si=JI6F7-VQyhIpxkHr\"\n",
    "model_path = \"translater\"\n",
    "\n",
    "# Download audio\n",
    "pipeline(youtube_url, model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
